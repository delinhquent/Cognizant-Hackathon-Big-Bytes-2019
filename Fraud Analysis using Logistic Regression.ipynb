{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Necessary functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call all necessary functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix,accuracy_score, fbeta_score\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintStats(y_test, pred, output_print=False):\n",
    "    f1Score = round(f1_score(y_test, pred), 2)\n",
    "    fbetaScore = round(fbeta_score(y_test, pred, beta=1.25))\n",
    "    recallScore = round(recall_score(y_test, pred), 2)\n",
    "    precscore = round(precision_score(y_test, pred), 2)\n",
    "    accScore = round(accuracy_score(y_test, pred), 2)\n",
    "    \n",
    "    #roc curve\n",
    "    auc = PlotROC(y_test,pred)\n",
    "    \n",
    "    print(\"Accuracy for Model : {acc_score}\".format(acc_score = accScore))\n",
    "    print(\"Precision for Model : {prec_score}\".format(prec_score = precscore))\n",
    "    print(\"Sensitivity/Recall for Model : {recall_score}\".format(recall_score = recallScore))\n",
    "    print(\"F1 Score for Model : {f1_score}\".format(f1_score = f1Score))\n",
    "    print(\"F-Beta Score for Model : {fbeta_score}\".format(fbeta_score = fbetaScore))\n",
    "    \n",
    "    output = [accScore,precscore,recallScore,f1Score, fbetaScore,auc]\n",
    "    \n",
    "    if output_print:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Random Oversampling Method & Other Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oversample(X_train,Y_train,print_output=False):\n",
    "    Train_set = np.concatenate((X_train, Y_train), axis=1)\n",
    "\n",
    "    #Convert back to dataframe for random oversampling\n",
    "    df = pd.DataFrame.from_records(Train_set)\n",
    "\n",
    "    # Class count\n",
    "    count_class_0, count_class_1 = df.iloc[:,30].value_counts()\n",
    "\n",
    "    # Divide by class\n",
    "    df_class_0 = df[df.iloc[:,30] == 0]\n",
    "    df_class_1 = df[df.iloc[:,30] == 1]\n",
    "\n",
    "    df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "    df = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "    #shuffle rows\n",
    "    df.sample(frac=1)\n",
    "\n",
    "    # df_test_over.Class.value_counts().plot(kind='bar', title='Count (target)');\n",
    "    Y_train = df.iloc[:,30].values\n",
    "    X_train = df.iloc[:,0:30].values\n",
    "    \n",
    "    if print_output == True:\n",
    "        print('Random over-sampling:')\n",
    "        print(df.iloc[:,30].value_counts())\n",
    "    \n",
    "    return X_train, Y_train\n",
    "\n",
    "def Convert_prob_to_class(Y_test_hat):\n",
    "    Y_test_hat[Y_test_hat > 0.5] = 1\n",
    "    Y_test_hat[Y_test_hat < 0.5] = 0\n",
    "\n",
    "def PlotROC(y_test,pred):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Set Dataframe </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard-training set v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Renaming Columns</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "new_names =  {'Seconds since reference time': 'Time',\n",
    "'Amount': 'Amount',\n",
    "'Fraud? (1: Fraud, 0:  No Fraud)': 'Class',\n",
    "'Feature 1': 'F1',\n",
    "'Feature 2': 'F2',\n",
    "'Feature 3': 'F3',\n",
    "'Feature 4': 'F4',\n",
    "'Feature 5': 'F5',\n",
    "'Feature 6': 'F6',\n",
    "'Feature 7': 'F7',\n",
    "'Feature 8': 'F8',\n",
    "'Feature 9': 'F9',\n",
    "'Feature 10': 'F10',\n",
    "'Feature 11': 'F11',\n",
    "'Feature 12': 'F12',\n",
    "'Feature 13': 'F13',\n",
    "'Feature 14': 'F14',\n",
    "'Feature 15': 'F15',\n",
    "'Feature 16': 'F16',\n",
    "'Feature 17': 'F17',\n",
    "'Feature 18': 'F18',\n",
    "'Feature 19': 'F19',\n",
    "'Feature 20': 'F20',\n",
    "'Feature 21': 'F21',\n",
    "'Feature 22': 'F22',\n",
    "'Feature 23': 'F23',\n",
    "'Feature 24': 'F24',\n",
    "'Feature 25': 'F25',\n",
    "'Feature 26': 'F26',\n",
    "'Feature 27': 'F27',\n",
    "'Feature 28': 'F28',\n",
    "'Feature 29': 'F29'}\n",
    "df.rename(columns=new_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generate Chart</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this next to check graph\n",
    "for row in df.head():\n",
    "    fig=plt.figure(figsize=(17,10))\n",
    "    df.hist(column=row)\n",
    "    plt.xlabel(row,fontsize=15)\n",
    "    plt.ylabel(\"Frequency\",fontsize=15)\n",
    "\n",
    "# From these charts, we can see that the distribution for feature 23 is considered uniform.\n",
    "# It is meaningless and would introduce noisy outputs to our machine learning model, which is why we would drop it later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Check for duplications & % of Null Values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicated rows\n",
    "count = 0\n",
    "for rows in df.duplicated():\n",
    "    if rows == True:\n",
    "        count += 1\n",
    "if count > 0:\n",
    "    print(\"There are\",count,\"duplicated rows\")\n",
    "else:\n",
    "    print(\"There are no duplicated rows\")\n",
    "\n",
    "#check for null values\n",
    "total = df.isnull().sum().sort_values(ascending = False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)\n",
    "pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Analyze dataset based on classes</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density Plot for Credit Card Transactions Time\n",
    "for i in range(2):\n",
    "    subset = df[df['Class'] == i]\n",
    "    # Draw the density plot\n",
    "    sns.distplot(subset['Time'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = i)\n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 16}, title = 'Fraud')\n",
    "plt.title('Credit Card Transactions Time Density Plot')\n",
    "plt.xlabel('Seconds since reference time')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Box Plot for Fraud Transactions\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "s = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=True).set_title(\"Box Plot with Outliers\")\n",
    "s = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=False).set_title(\"Box Plot without Outliers\")\n",
    "plt.show();\n",
    "\n",
    "# Desnsity Plot for distribution of each features for each classes\n",
    "var = df.columns.values\n",
    "\n",
    "i = 0\n",
    "t0 = df.loc[df['Class'] == 0]\n",
    "t1 = df.loc[df['Class'] == 1]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(8,4,figsize=(16,28))\n",
    "\n",
    "for feature in var:\n",
    "    if feature != \"Class\":\n",
    "        i += 1\n",
    "        plt.subplot(8,4,i)\n",
    "        sns.kdeplot(t0[feature], bw=0.5,label=\"Fraud = 0\")\n",
    "        sns.kdeplot(t1[feature], bw=0.5,label=\"Fraud = 1\")\n",
    "        plt.xlabel(feature, fontsize=12)\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='both', which='major', labelsize=9)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Format properly with dropping rows which contains null values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows which has null values\n",
    "df = df.dropna()\n",
    "\n",
    "#drop feature 23\n",
    "df.drop(['F23'], inplace=True, axis=1)\n",
    "columns = [\"Time\",\"F1\",\"F2\",\"F3\",\"F4\",\"F5\",\"F6\",\"F7\",\"F8\",\"F9\",\"F10\",\"F11\",\"F12\",\"F13\",\"F14\",\"F15\",\"F16\",\"F17\",\"F18\",\"F19\",\"F20\",\"F21\",\"F22\",\"F24\",\"F25\",'F26','F27',\"F28\",\"F29\",\"Amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Split into training and testing data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this to split into train and test data\n",
    "X = pd.DataFrame.as_matrix(df,columns=columns)\n",
    "Y = df.Class\n",
    "Y = Y.values.reshape(Y.shape[0],1)\n",
    "X.shape\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "#oversampling\n",
    "X_train, Y_train = Oversample(X_train,Y_train)\n",
    "\n",
    "model = LogisticRegression(C=1)\n",
    "\n",
    "model.fit(X_train, Y_train.ravel())\n",
    "\n",
    "Y_test_hat = model.predict(X_test)\n",
    "Convert_prob_to_class(Y_test_hat)\n",
    "\n",
    "#Print metrics \n",
    "PrintStats(Y_test,Y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cross-Validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters for k-fold cv\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits)\n",
    "Metric_array = np.zeros(6)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#Start k-fold cross validation \n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    #Split into training and testing set\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    #Oversampling of training set ONLY\n",
    "    X_train, Y_train = Oversample(X_train,Y_train)\n",
    "    \n",
    "    #run the model\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    #Run forward prop to get predicted values\n",
    "    Y_test_hat = model.predict(X_test)\n",
    "    Convert_prob_to_class(Y_test_hat)\n",
    "    \n",
    "    print('\\nMetrics for fold number: ' + str(counter))\n",
    "    counter += 1\n",
    "    \n",
    "    #Print metrics and store in numpy array to average after end of cv \n",
    "    Metric_array += PrintStats(Y_test,Y_test_hat,True)\n",
    "    \n",
    "#Averaging of metrics    \n",
    "Metric_array /= n_splits\n",
    "np.around(Metric_array, decimals=2)\n",
    "\n",
    "print('\\nSummary metrics:')\n",
    "print(\"Accuracy for Model : {acc_score}\".format(acc_score = Metric_array[0]))\n",
    "print(\"Precision for Model : {prec_score}\".format(prec_score = Metric_array[1]))\n",
    "print(\"Sensitivity/Recall for Model : {recall_score}\".format(recall_score = Metric_array[2]))\n",
    "print(\"F1 Score for Model : {f1_score}\".format(f1_score = Metric_array[3]))\n",
    "print(\"F-Beta Score for Model : {f1_score}\".format(f1_score = Metric_array[4]))\n",
    "print(\"AUC for Model : {auc}\".format(auc = Metric_array[5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
